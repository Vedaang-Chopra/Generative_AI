{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-dotenv\n",
    "# ! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Langchain\n",
    "<br> \n",
    "It is an opensource development framework for LLM Applications focused on composition and modularity and also chaining these components. \n",
    "<br> It has a python and a javascript version.\n",
    "<br> It is highly modular and can be used to build any kind of LLM application. Consists of chains to connect different modules. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Topics to learn: - \n",
    "Models : - It can connect to different LLM's like GPT-3, BERT, Mistral, OLLAMA etc.\n",
    "Prompts: - It has different prompt templates, and retry and fixing logics.\n",
    "Indexes :- It can take input from different sources like text, image, audio, and connects to different retreivers and vector stores etc.\n",
    "Chains: - Connecting the Input the LLM and the output throught different mechanism is chaining. \n",
    "Agents :- Making the LLM more interactive and giving it reasoning abilities is agents. (Now done with langgraph)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Ollama for development\n",
    "#### This is a demo test, testing the LLM Model using OLLAMA, for basic chat operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the Ollama LLM\n",
    "ollama_llm = Ollama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 2. Define a Prompt Template (with placeholders for dynamic variables)\n",
    "# prompt_template = PromptTemplate(\n",
    "#     input_variables=[\"topic\"],\n",
    "#     template=\"You are an expert on {topic}. Please describe it in simple terms.\"\n",
    "# )\n",
    "\n",
    "# # 3. Build the chain using the prompt template and the Ollama LLM\n",
    "# chain = LLMChain(llm=ollama_llm, prompt=prompt_template)\n",
    "\n",
    "# # 4. Run the chain by passing in the required 'topic' input variable\n",
    "# result = chain.run(topic=\"quantum mechanics\")\n",
    "# print(\"Generated Response:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# response: ChatResponse = chat(model='llama3.1', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# print(response['message']['content'])\n",
    "# # or access fields directly from the response object\n",
    "# print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "## Outline\n",
    "\n",
    " * Direct API calls to OpenAI\n",
    " * API calls through LangChain:\n",
    "   * Prompts:-  It refers to the text input that is used to generate output from the model\n",
    "   * Models : - It referes to the LLM Model, which is a pre-trained model that can be used to generate text\n",
    "   * Output parsers : - It refers to the parsing the output of LLM's for a more structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
